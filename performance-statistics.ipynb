{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we teach useful statistics for performance evaluation?\n",
    "\n",
    "## Confidence Interval Computation\n",
    "\n",
    "Often confidence interval computatoins rely on the fact that the sample mean distribution is asymptomaticaly normal.\n",
    "\n",
    "More formally for a confidence interval from $r$ runs at confidence level $ 1 - \\alpha$.\n",
    "\n",
    "1. Collect many more measurements than we use to compute the confidence interval.\n",
    "   For peformance that may be set runs $M = \\{R_1, R_2,..., R_m\\}$ where each run (e.g. of a benchmark) is set of samples (iteration results) $R_i = \\{s_1^i, s_2^i, ..., s_n^i\\}$.\n",
    "2. We set $\\bar{M} = \\frac{\\sum_{i=1}^{m} \\sum_{j=1}^{n} s_j^i}{m n}$ as the sample mean of all measurements.\n",
    "3. We draw a random subset $M^{ci} \\subset M$ with replacement such that $|M^{ci}| = r$ and compute the confidence interval $ci$ from $M^{ci}$.\n",
    "4. We repeat previous step  multiple times and keep track of how many times $\\bar{M} \\in ci$.\n",
    "5. Using the count from above, we compute probability $P(\\bar{M} \\in ci)$; ideally $P(\\bar{M} \\in ci) \\approx 1 - \\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/drozt/d/school/statistics/performance-statistics.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drozt/d/school/statistics/performance-statistics.ipynb#ch0000003?line=10'>11</a>\u001b[0m r \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drozt/d/school/statistics/performance-statistics.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m bootstrap_i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/drozt/d/school/statistics/performance-statistics.ipynb#ch0000003?line=12'>13</a>\u001b[0m     sample \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(M, size\u001b[39m=\u001b[39;49mr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/drozt/d/school/statistics/performance-statistics.ipynb#ch0000003?line=13'>14</a>\u001b[0m     stats\u001b[39m.\u001b[39mt\u001b[39m.\u001b[39minterval()\n",
      "File \u001b[0;32mmtrand.pyx:911\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "pop_mean = 123\n",
    "pop_std = 7\n",
    "\n",
    "m = 1000\n",
    "n = 10\n",
    "M = [np.random.normal(pop_mean, pop_std, size=n) for _ in range(m)]\n",
    "\n",
    "M_bar = sum([np.sum(R_i) for R_i in M]) / (m * n)\n",
    "\n",
    "m_bar_hit = 0\n",
    "r = 10\n",
    "for bootstrap_i in range(100):\n",
    "    sample = np.random.choice(M, size=r)\n",
    "    stats.t.interval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing\n",
    "\n",
    "Common statistical hypothesis tests used to detect difference in performance are:\n",
    "\n",
    "1. **Welch's t-test**\n",
    "    * assumes normally distributed data, but is consedered robust enough to be used with other probability distributions.\n",
    "    * $H_0$: two compared distributions have equal means.\n",
    "\n",
    "2. **Mann-Whiteney U-test**\n",
    "    * does not assume any particular probability distribution\n",
    "    * $H_0$: A sample drawn from one of the two compared distributions is as likely to exceed a sample drawn from the other distribution as the other way around.\n",
    "\n",
    "When the test assumptions are met both tests should reject the null hypothesis, or make Type 1 error, with probability equal to the significance level.\n",
    "\n",
    "When comparing measurements of the same benchmark the null hypothesis should hold by definition.\n",
    "The rejection rate would therefore ideally approach the significance level.\n",
    "\n",
    "More formally for hypothesis test from $r$ runs at significance level $\\alpha$, when a measurements are represented as a set of run $M = \\{R_1, R_2, ..., R_m\\}$ and each run is represented as a set of samples $R_i = \\{s_1^i, s_2^i, ..., s_n^i\\}$ we compute as follows:\n",
    "\n",
    "1. we draw random subsets $M^a, M^b \\subset M$ with replacement such that $|M^a| = |M^b| = r$ and evaluate the hypotehsis test between $M^a, M^b$.\n",
    "2. repeat previous step and keep track of how many times the hypothesis test rejected the null hypothesis\n",
    "3. using hte count from above, we compute the hypothesis rejection probability $P$; ideally $P \\approx 1 - alpha$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6829942a0b2a8f8b45ae2f23d08786da59ba5386601097735a129dc87f734f4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
